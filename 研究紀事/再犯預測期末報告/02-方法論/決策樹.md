決策樹(Decision Tree [1] [1])是一種監督式的資料分類技術，為隨機森林的核心模型。本研究所使用的分類樹模型為分類回歸樹(Classification and Regression Tree, CART [2] [2])，CART將所有預測因子變數值所組成的向量空間（稱為「特徵空間 」）以遞歸的方式分割，每次分割針對某一個影響因子，根據其值分割，將具有相似分類結果的資料分為一組，分割完成後，在每個群組內具有較為一致的預測分類結果。

### Gini值
分割時會依據節點中的雜度值(Gini, G) [3] [3] 來評估分割的效率，G=0表示節點中所有的資料都屬於同一種類型，G=1表示節點中的資料每一種分類都有。Gini函數的定義如下：$$G=1-\sum_{k=1}^n(p_k^2)$$G 為gini值，n種分類，$p_k$ 代表第k種分類的資料在節點中的比例。例如，在一個節點中有10筆資料分為2類，第一類有3筆資料($p_1=\frac{3}{10}$，第二類有7筆資料($p_2=\frac{7}{10}$)，
$$
\begin{align*}
G&=1-\sum_{k=1}^{2}p_k^2=1-p_1^2-p_2^2=1-\frac{3}{10}^2-\frac{7}{10}^2=\frac{42}{100}
\end{align*}
$$
每一次分隔時，儘可能找到適當的特徵因子及其分割值，使其結果的節點Gini值最大。

### CART建立流程
由以下四個步驟組成：
1. 使用二元遞迴分割法(Recursively Binary Partitioning)建立模型：分類樹建立過程由跟節點開始，CART演算法找尋最佳可能的預測變數進行分割成兩個子節點，根據最佳可能的預測變數，並依據變數值進行分割。找尋最佳分割變數時，CART根據找尋到的預測分割變數使產生的兩子節點的資料純度最大。建立分類樹的流程包括節點分割，為每一個節點指定分類類別，此一流程將不斷地演進，一直到不能繼續為止。
2. 建樹流程的停止規則：CART的建立過程一直不斷運用二元遞迴分割法進行，停止建立CART有以下三個條件，
		(1)每一個子節點中只有一筆資料，
		(2)每一個子節點中所有資料的預測變數具相同的分佈，及
		(3)使用者指定樹的最大階層數。
3. 修剪樹節點，使分類樹較為簡單：為要獲得更簡單的分類樹模型可以利用「成本複雜度」(cost-complexity)修剪方法，方法中需要一個複雜參數($\alpha$)，$\alpha$ 在分類樹修剪的過程中會漸漸地增加。$\alpha$ 是一個量度，主要量測將一個分割加入分類樹後所產生的額外準確性。當 $\alpha$ 增加，更多的節點將被剪除，形成更簡潔的分類樹。
4. 尋求最佳分類樹：最大分類樹(maximal tree)擬合(fit)學習資料集獲得較高的準確性，但也會產生過度擬合(overfitting)問題，即maximal tree擬合學習資料集中資料的每一個特性及雜訊，但對不同的學習資料集，就無法充分擬合，而導致預測不準確。解決的方法是尋找正確的複雜參數($\alpha$)，使分類樹模型針對學習資料集進行擬合，但不過度擬合。因此，可以另用額外的測試資料集，針對學習資料集來調整a參數，當節點數目增加時，就能降低決策成本。反之，針對測試資料集，當減少複雜度，決策成本降低到某一個最小值時，接下去就會開始上升。

### CART的優點與缺點 [4] [4]

CART的優點如下
* 容易理解與視覺化表示
* CART演算中包括影響因子的找尋及選擇
* 不僅能針對數值及類別型資料，也可以處理多類別分類問題
* 資料準備的工作較少
* 參數間非線性關聯性不會影響分類樹的效能

CART的缺點如下
* 可能產生過度擬合的問題
* 資料的微小變異可能產生完全不同的分類樹，影響其穩定性
* 有可能無法找到全域最佳決策樹
* 分類資料分佈不平均時會產生偏差的決策樹

### reference
[1]: R. J. Lewis, “An Introduction to Classification and Regression Tree (CART) Analysis,” in _Annual Meeting of the Society for Academic Emergency Medicine_, San  Francisco, California, 2000, p. 14.
[2]: R. Timofeev, “Classiﬁcation and Regression Trees (CART) Theory and Applications,” Master Thesis, Humboldt University, Berlin, 2004.
[3]: L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen, _Classification and Regression Trees_. Boca Raton, Fla., 1984.
[4]: P. Gupta, “Decision Trees in Machine Learning,” _Medium_, Nov. 12, 2017. [https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) (accessed Jul. 12, 2023).

