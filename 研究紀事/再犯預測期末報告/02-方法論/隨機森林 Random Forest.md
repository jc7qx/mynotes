隨機森林(Random Forest) [^1]是以分類與回歸決策樹(CART)為基礎的監督式機器學習演算法，隨機森林演算法根據決策樹的預測結果，它集合許多分類樹來解決複雜分類問題，增加分類樹的數目可以提昇結果的精確度。隨機森林演算法依賴於多顆決策樹。每一個決策樹都是由決策節點、葉節點和根節點組成。每棵樹的葉節點是該決策樹產生的輸出結果。最終輸出的結果遵循多數投票制。在這種情況下，集合大多數決策樹產生的輸出成為隨機森林模型的最後輸出。

利用多顆決策樹獲得預測分類結果的方式被稱為集成式學習(Ensemble Learning Methods)，最著名的集成式方法為Bagging，又稱為Boostrap Aggregation，隨機森林演算法是Bagging的延伸，它不僅由學習資料重複隨機抽樣出資料，在每次分類樹進行分割作業時隨機抽取部分的影響因子變數，在這些抽取的影響因子變數中找出適當的影響因子予以分割，而傳統分類樹則是考量每一個影響因子變數使其成為適當的分割變數。隨機森林演算法考量資料的所有可能變化性，故可降低過度擬合(Overfitting)的風險、分類樹的偏差、及整體的變異性，最後能獲得更精確的預測。隨機森林優點包括回歸及分類預測、可獲得精確的結果，並解釋性佳、有效率地處理大量資料集、及比分類樹的預測結果更為精確。圖 xxx 顯示隨機森林分類器的概念。
![[Pasted image 20230713111949.png]]

**隨機森林演算法

1. 決定決策樹的數目(B)執行以下步驟
```
	for b=1 to B {
		(1) 由訓練資料集隨機抽選N筆資料（Z*）
		(2) 依據Z*及以下規則建立決策樹
			(a) 由影響因子中隨機選取m個變數
			(b) 針對m個影響因子，尋找每個因子的分割
			(c) 每個節點分割成2個子節點
	}
```
2. 依據上述步驟產生B棵決策樹集合($T_{b=1}^B$)
3. 預測分類結果：令$\widehat{C_{bi}^B}(x)$，而$\widehat{C_{rf}^B}(x)$代表由B顆樹投票得到的結果，即
$$\widehat{C_{rf}^B}(x)={\color{red} majority \ vote\  } \widehat{C_b}(x)_{b=1}^B$$
隨機森林優點包括
* 可以執行回歸及分類工作
* 可以獲得精確的結果，並解釋性佳
* 可以有效率地處理大量資料集
* 比決策分類樹的預測結果更為精確

隨機森林缺點包括
* 消耗較多的計算資源
* 運算耗費較多時間

[^1]: Trevor Hastie, Robert Tibshirani, and Jerome Friedman, _The Elements of Statistical Learning_, 2nd ed. Springer New York, 2009.